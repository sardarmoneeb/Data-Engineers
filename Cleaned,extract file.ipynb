{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b828ac88-0767-4fcf-b1db-5bfae8833868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines before cleaning: 58\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Step 1: Read the raw text file\n",
    "with open(\"C:/Users/LENOVE/Desktop/Data Engineers/raw_messy_text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.readlines()\n",
    "\n",
    "print(f\"Total lines before cleaning: {len(raw_text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5645cd56-b832-4815-855e-8e4a45dc6a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Remove empty lines and strip extra spaces\n",
    "cleaned = [line.strip() for line in raw_text if line.strip() != \"\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "154be870-6784-4bf0-9e65-e94c13e5ee50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Remove HTML tags\n",
    "cleaned = [re.sub(r\"<.*?>\", \"\", line) for line in cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8789f4ed-669e-4c6e-9b66-9eeccd619df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Normalize multiple spaces and tabs\n",
    "cleaned = [re.sub(r\"\\s+\", \" \", line) for line in cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3f0633f6-d537-4075-8fd4-af09a76dbb1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cleaning complete!\n",
      "Total lines after cleaning: 36\n",
      "Cleaned file saved as: cleaned_text.txt\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Remove duplicate lines\n",
    "cleaned = list(dict.fromkeys(cleaned))\n",
    "\n",
    "# Step 6: Remove random punctuation clutter (like ...., !!!, --- etc.)\n",
    "cleaned = [re.sub(r\"[.]{2,}|[!?]{2,}|[-=]{2,}\", \".\", line) for line in cleaned]\n",
    "\n",
    "# Step 7: (Optional) Remove lines with unwanted symbols or noise\n",
    "# e.g. remove lines starting with '---', '>>>', or containing broken JSON\n",
    "cleaned = [line for line in cleaned if not re.match(r\"^[-=]{2,}|^>>>\", line)]\n",
    "cleaned = [line for line in cleaned if not re.search(r\"Broken JSON\", line)]\n",
    "\n",
    "# Step 8: Save the cleaned output to a new text file\n",
    "with open(\"C:/Users/LENOVE/Desktop/Data Engineers/cleaned_text.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for line in cleaned:\n",
    "        f.write(line + \"\\n\")\n",
    "\n",
    "print(\"‚úÖ Cleaning complete!\")\n",
    "print(f\"Total lines after cleaning: {len(cleaned)}\")\n",
    "print(\"Cleaned file saved as: cleaned_text.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "719487f8-9f31-4805-a364-3a934ea4d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Load cleaned text\n",
    "with open(\"C:/Users/LENOVE/Desktop/Data Engineers/cleaned_text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44cd457b-a6ed-47fa-930f-f6a81c4c7285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Emails found: ['john.doe@example.com', 'jane_smith@domain.co.uk']\n"
     ]
    }
   ],
   "source": [
    "emails = re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", text)\n",
    "print(\"Emails found:\", emails)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b9091e8-bca7-4c3c-a3ba-c964c3f53cb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phone numbers found: ['2025-10-01 08', '2025-10-02 09', '10\\n2025-10-02 09', '+1 (555) 123-4567\\n555.123.4568']\n"
     ]
    }
   ],
   "source": [
    "phones = re.findall(r\"\\+?\\d[\\d().\\-\\s]{7,}\\d\", text)\n",
    "print(\"Phone numbers found:\", phones)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eded19db-2979-4992-9f66-64ad8bf11543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URLs found: ['https://example.com/page?ref=abc', 'http://localhost:8000/test']\n"
     ]
    }
   ],
   "source": [
    "urls = re.findall(r\"https?://[^\\s]+\", text)\n",
    "print(\"URLs found:\", urls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2914e14c-ace5-44eb-bd5b-00f7f18d3656",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dates found: ['2025-10-01', '2025-10-02', '2025-10-02']\n",
      "Times found: ['08:15:23', '09:00:00', '09:00:01']\n"
     ]
    }
   ],
   "source": [
    "dates = re.findall(r\"\\d{4}-\\d{2}-\\d{2}\", text)\n",
    "times = re.findall(r\"\\d{2}:\\d{2}:\\d{2}\", text)\n",
    "\n",
    "print(\"Dates found:\", dates)\n",
    "print(\"Times found:\", times)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ad07bc32-c5a8-4dd8-8ee1-d8cabda9d486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extraction complete! File saved as extracted_data.txt\n"
     ]
    }
   ],
   "source": [
    "with open(\"C:/Users/LENOVE/Desktop/Data Engineers/extracted_data.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Emails:\\n\" + \"\\n\".join(emails) + \"\\n\\n\")\n",
    "    f.write(\"Phone Numbers:\\n\" + \"\\n\".join(phones) + \"\\n\\n\")\n",
    "    f.write(\"URLs:\\n\" + \"\\n\".join(urls) + \"\\n\\n\")\n",
    "    f.write(\"Dates:\\n\" + \"\\n\".join(dates) + \"\\n\\n\")\n",
    "    f.write(\"Times:\\n\" + \"\\n\".join(times) + \"\\n\\n\")\n",
    "\n",
    "print(\"‚úÖ Extraction complete! File saved as extracted_data.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab057397-ec6b-4207-8a24-2d45a2e4be16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data extracted and saved as extracted_summary.csv!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "\n",
    "# Step 1: Load the text file\n",
    "with open(\"cleaned_text.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "\n",
    "# Step 2: Define regex patterns for extraction\n",
    "emails = re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", text)\n",
    "phones = re.findall(r\"\\+?\\d[\\d().\\-\\s]{7,}\\d\", text)\n",
    "urls = re.findall(r\"https?://[^\\s]+\", text)\n",
    "dates = re.findall(r\"\\d{4}-\\d{2}-\\d{2}\", text)\n",
    "times = re.findall(r\"\\d{2}:\\d{2}:\\d{2}\", text)\n",
    "\n",
    "# Step 3: Prepare to save extracted data\n",
    "# (We'll pad lists so all columns have same length)\n",
    "max_len = max(len(emails), len(phones), len(urls), len(dates), len(times))\n",
    "def pad(lst):\n",
    "    return lst + [\"\"] * (max_len - len(lst))\n",
    "\n",
    "emails, phones, urls, dates, times = map(pad, [emails, phones, urls, dates, times])\n",
    "\n",
    "# Step 4: Save into CSV\n",
    "with open(\"C:/Users/LENOVE/Desktop/Data Engineers/extracted_summary.csv\", \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"Email\", \"Phone\", \"URL\", \"Date\", \"Time\"])\n",
    "    for i in range(max_len):\n",
    "        writer.writerow([\n",
    "            emails[i],\n",
    "            phones[i],\n",
    "            urls[i],\n",
    "            dates[i],\n",
    "            times[i]\n",
    "        ])\n",
    "\n",
    "print(\"‚úÖ Data extracted and saved as extracted_summary.csv!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecc55079-51f7-4b34-b9dd-0c222aed3828",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full path being used: C:\\Users\\LENOVE\\Desktop\\Data Engineers\\Python\\Text clean file\n",
      "Files detected: ['.ipynb_checkpoints', 'Cleaned,extract file.ipynb', 'cleaned_text.txt', 'extracted_data.txt', 'extracted_summary.csv', 'raw_messy_text.txt']\n",
      "\n",
      "üîç Starting extraction process...\n",
      "\n",
      "\n",
      "Processing: C:/Users/LENOVE/Desktop/Data Engineers/Python/Text clean file\\raw_messy_text.txt\n",
      "File size: 1172\n",
      "File preview: ERROR: 2025-10-01 08:15:23 - Failed to connect to DB (timeout)\n",
      "User:  john.doe@example.com   Action: login\n",
      "----\n",
      "<html><body><h1>Report</h1><p>Sales increased by 5%.</p></body></html>\n",
      "\n",
      "Todo:   fix,norm\n",
      "Emails found: ['john.doe@example.com', 'jane_smith@domain.co.uk', 'jane_smith@domain.co.uk']\n",
      "Phones found: ['2025-10-01 08', '2025-10-02 09', '10\\n2025-10-02 09', '+1 (555) 123-4567\\n555.123.4568']\n",
      "URLs found: ['https://example.com/page?ref=abc', 'http://localhost:8000/test']\n",
      "Dates found: ['2025-10-01', '2025-10-02', '2025-10-02']\n",
      "Times found: ['08:15:23', '09:00:00', '09:00:01']\n",
      "‚úÖ Data extracted and saved to: C:/Users/LENOVE/Desktop/Data Engineers/Python/Text clean file\\raw_messy_text_extracted.csv\n",
      "\n",
      "üéØ All eligible files processed successfully!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import os\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "input_folder = r\"C:/Users/LENOVE/Desktop/Data Engineers/Python/Text clean file\"\n",
    "output_folder = r\"C:/Users/LENOVE/Desktop/Data Engineers/Python/Text clean file\"\n",
    "\n",
    "# Create folders if not exist\n",
    "os.makedirs(input_folder, exist_ok=True)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# === FUNCTION TO EXTRACT DATA FROM TEXT ===\n",
    "def extract_from_text(text):\n",
    "    emails = re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", text)\n",
    "    phones = re.findall(r\"\\+?\\d[\\d().\\-\\s]{7,}\\d\", text)\n",
    "    urls = re.findall(r\"https?://[^\\s]+\", text)\n",
    "    dates = re.findall(r\"\\d{4}-\\d{2}-\\d{2}\", text)\n",
    "    times = re.findall(r\"\\d{2}:\\d{2}:\\d{2}\", text)\n",
    "\n",
    "    print(\"Emails found:\", emails)\n",
    "    print(\"Phones found:\", phones)\n",
    "    print(\"URLs found:\", urls)\n",
    "    print(\"Dates found:\", dates)\n",
    "    print(\"Times found:\", times)\n",
    "\n",
    "    max_len = max(len(emails), len(phones), len(urls), len(dates), len(times), 1)\n",
    "    def pad(lst): return lst + [\"\"] * (max_len - len(lst))\n",
    "    return pad(emails), pad(phones), pad(urls), pad(dates), pad(times)\n",
    "\n",
    "# === MAIN PIPELINE ===\n",
    "print(\"Full path being used:\", os.path.abspath(input_folder))\n",
    "print(\"Files detected:\", os.listdir(input_folder))\n",
    "print(\"\\nüîç Starting extraction process...\\n\")\n",
    "\n",
    "for filename in os.listdir(input_folder):\n",
    "    if filename.endswith(\".txt\") and \"cleaned\" not in filename.lower() and \"extract\" not in filename.lower():\n",
    "        file_path = os.path.join(input_folder, filename)\n",
    "        print(f\"\\nProcessing: {file_path}\")\n",
    "\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            text = f.read()\n",
    "\n",
    "        print(\"File size:\", len(text))\n",
    "        print(\"File preview:\", text[:200])\n",
    "\n",
    "        emails, phones, urls, dates, times = extract_from_text(text)\n",
    "\n",
    "        csv_name = filename.replace(\".txt\", \"_extracted.csv\")\n",
    "        csv_path = os.path.join(output_folder, csv_name)\n",
    "\n",
    "        with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerow([\"Email\", \"Phone\", \"URL\", \"Date\", \"Time\"])\n",
    "            for i in range(len(emails)):\n",
    "                writer.writerow([emails[i], phones[i], urls[i], dates[i], times[i]])\n",
    "\n",
    "        if any([emails, phones, urls, dates, times]):\n",
    "            print(f\"‚úÖ Data extracted and saved to: {csv_path}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è No matches found in: {filename}\")\n",
    "\n",
    "print(\"\\nüéØ All eligible files processed successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eca359b4-0717-4e75-bee9-1045fbe1204b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: schedule in c:\\users\\lenove\\appdata\\roaming\\python\\python312\\site-packages (1.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a8c634-1263-4abf-8914-57a029e49df8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è∞ Automation started! The extraction will run every 5 minutes.\n",
      "\n",
      "\n",
      "===============================\n",
      "üöÄ Running automated extraction job...\n",
      "===============================\n",
      "\n",
      "Processing: C:/Users/LENOVE/Desktop/Data Engineers/Python/Text clean file\\raw_messy_text.txt\n",
      "‚úÖ File saved/updated: C:/Users/LENOVE/Desktop/Data Engineers/Python/Text clean file\\raw_messy_text_extracted.csv\n",
      "üéØ 1 file(s) processed successfully!\n",
      "\n",
      "\n",
      "===============================\n",
      "üöÄ Running automated extraction job...\n",
      "===============================\n",
      "\n",
      "Processing: C:/Users/LENOVE/Desktop/Data Engineers/Python/Text clean file\\raw_messy_text.txt\n",
      "‚úÖ File saved/updated: C:/Users/LENOVE/Desktop/Data Engineers/Python/Text clean file\\raw_messy_text_extracted.csv\n",
      "üéØ 1 file(s) processed successfully!\n",
      "\n",
      "\n",
      "===============================\n",
      "üöÄ Running automated extraction job...\n",
      "===============================\n",
      "\n",
      "Processing: C:/Users/LENOVE/Desktop/Data Engineers/Python/Text clean file\\raw_messy_text.txt\n",
      "‚úÖ File saved/updated: C:/Users/LENOVE/Desktop/Data Engineers/Python/Text clean file\\raw_messy_text_extracted.csv\n",
      "üéØ 1 file(s) processed successfully!\n",
      "\n",
      "\n",
      "===============================\n",
      "üöÄ Running automated extraction job...\n",
      "===============================\n",
      "\n",
      "Processing: C:/Users/LENOVE/Desktop/Data Engineers/Python/Text clean file\\raw_messy_text.txt\n",
      "‚úÖ File saved/updated: C:/Users/LENOVE/Desktop/Data Engineers/Python/Text clean file\\raw_messy_text_extracted.csv\n",
      "üéØ 1 file(s) processed successfully!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import csv\n",
    "import os\n",
    "import time\n",
    "import schedule\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "input_folder = r\"C:/Users/LENOVE/Desktop/Data Engineers/Python/Text clean file\"\n",
    "output_folder = r\"C:/Users/LENOVE/Desktop/Data Engineers/Python/Text clean file\"\n",
    "\n",
    "# Create folders if not exist\n",
    "os.makedirs(input_folder, exist_ok=True)\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# === FUNCTION TO EXTRACT DATA FROM TEXT ===\n",
    "def extract_from_text(text):\n",
    "    emails = re.findall(r\"[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}\", text)\n",
    "    phones = re.findall(r\"\\+?\\d[\\d().\\-\\s]{7,}\\d\", text)\n",
    "    urls = re.findall(r\"https?://[^\\s]+\", text)\n",
    "    dates = re.findall(r\"\\d{4}-\\d{2}-\\d{2}\", text)\n",
    "    times = re.findall(r\"\\d{2}:\\d{2}:\\d{2}\", text)\n",
    "\n",
    "    max_len = max(len(emails), len(phones), len(urls), len(dates), len(times), 1)\n",
    "    def pad(lst): return lst + [\"\"] * (max_len - len(lst))\n",
    "    return pad(emails), pad(phones), pad(urls), pad(dates), pad(times)\n",
    "\n",
    "# === MAIN PIPELINE FUNCTION ===\n",
    "def run_pipeline():\n",
    "    print(\"\\n===============================\")\n",
    "    print(\"üöÄ Running automated extraction job...\")\n",
    "    print(\"===============================\\n\")\n",
    "\n",
    "    files_processed = 0\n",
    "\n",
    "    for filename in os.listdir(input_folder):\n",
    "        # ‚úÖ Process only .txt files that aren‚Äôt cleaned/extracted\n",
    "        if filename.endswith(\".txt\") and \"cleaned\" not in filename.lower() and \"extract\" not in filename.lower():\n",
    "            file_path = os.path.join(input_folder, filename)\n",
    "            print(f\"Processing: {file_path}\")\n",
    "\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "\n",
    "            emails, phones, urls, dates, times = extract_from_text(text)\n",
    "\n",
    "            # ‚úÖ Generate fixed output file name (overwrite if exists)\n",
    "            csv_name = filename.replace(\".txt\", \"_extracted.csv\")\n",
    "            csv_path = os.path.join(output_folder, csv_name)\n",
    "\n",
    "            with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "                writer = csv.writer(f)\n",
    "                writer.writerow([\"Email\", \"Phone\", \"URL\", \"Date\", \"Time\"])\n",
    "                for i in range(len(emails)):\n",
    "                    writer.writerow([emails[i], phones[i], urls[i], dates[i], times[i]])\n",
    "\n",
    "            print(f\"‚úÖ File saved/updated: {csv_path}\")\n",
    "            files_processed += 1\n",
    "\n",
    "    if files_processed == 0:\n",
    "        print(\"‚ö†Ô∏è No eligible text files found to process.\")\n",
    "    else:\n",
    "        print(f\"üéØ {files_processed} file(s) processed successfully!\\n\")\n",
    "\n",
    "# === SCHEDULING ===\n",
    "schedule.every(5).minutes.do(run_pipeline)  # Run every 5 minutes\n",
    "\n",
    "print(\"‚è∞ Automation started! The extraction will run every 5 minutes.\\n\")\n",
    "\n",
    "# Run once immediately at start\n",
    "run_pipeline()\n",
    "\n",
    "# Keep running indefinitely\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ff5c6a-9018-47f0-8312-e661b4f43adf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
